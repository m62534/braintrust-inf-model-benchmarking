import { ChatCompletionMessageParam, ChatCompletionTool, ChatCompletionToolChoiceOption, ChatCompletion } from 'openai/resources';
import { OpenAI } from 'openai';
import { z } from 'zod';

interface Score {
    name: string;
    score: number | null;
    metadata?: Record<string, unknown>;
    /**
     * @deprecated
     */
    error?: unknown;
}
type ScorerArgs<Output, Extra> = {
    output: Output;
    expected?: Output;
} & Extra;
type Scorer<Output, Extra> = (args: ScorerArgs<Output, Extra>) => Score | Promise<Score>;

interface CachedLLMParams {
    /**
     Model to use for the completion.
     Note: If using Azure OpenAI, this should be the deployment name..
     */
    model: string;
    messages: ChatCompletionMessageParam[];
    tools?: ChatCompletionTool[];
    tool_choice?: ChatCompletionToolChoiceOption;
    temperature?: number;
    max_tokens?: number;
    span_info?: {
        spanAttributes?: Record<string, string>;
    };
}
interface ChatCache {
    get(params: CachedLLMParams): Promise<ChatCompletion | null>;
    set(params: CachedLLMParams, response: ChatCompletion): Promise<void>;
}
type OpenAIAuth = {
    /** @deprecated Use the `client` option instead */
    openAiApiKey?: string;
    /** @deprecated Use the `client` option instead */
    openAiOrganizationId?: string;
    /** @deprecated Use the `client` option instead */
    openAiBaseUrl?: string;
    /** @deprecated Use the `client` option instead */
    openAiDefaultHeaders?: Record<string, string>;
    /** @deprecated Use the `client` option instead */
    openAiDangerouslyAllowBrowser?: boolean;
    /** @deprecated Use the `client` option instead */
    azureOpenAi?: AzureOpenAiAuth;
    client?: never;
} | {
    client: OpenAI;
    /** @deprecated Use the `client` option instead */
    openAiApiKey?: never;
    /** @deprecated Use the `client` option instead */
    openAiOrganizationId?: never;
    /** @deprecated Use the `client` option instead */
    openAiBaseUrl?: never;
    /** @deprecated Use the `client` option instead */
    openAiDefaultHeaders?: never;
    /** @deprecated Use the `client` option instead */
    openAiDangerouslyAllowBrowser?: never;
    /** @deprecated Use the `client` option instead */
    azureOpenAi?: never;
};
interface AzureOpenAiAuth {
    apiKey: string;
    endpoint: string;
    apiVersion: string;
}
declare global {
    var __inherited_braintrust_wrap_openai: ((openai: any) => any) | undefined;
    var __client: OpenAI | undefined;
}
declare const init: ({ client }?: {
    client?: OpenAI;
}) => void;

declare const modelGradedSpecSchema: z.ZodObject<{
    prompt: z.ZodString;
    choice_scores: z.ZodRecord<z.ZodString, z.ZodNumber>;
    model: z.ZodOptional<z.ZodString>;
    use_cot: z.ZodOptional<z.ZodBoolean>;
    temperature: z.ZodOptional<z.ZodNumber>;
}, "strip", z.ZodTypeAny, {
    prompt: string;
    choice_scores: Record<string, number>;
    model?: string | undefined;
    use_cot?: boolean | undefined;
    temperature?: number | undefined;
}, {
    prompt: string;
    choice_scores: Record<string, number>;
    model?: string | undefined;
    use_cot?: boolean | undefined;
    temperature?: number | undefined;
}>;
type ModelGradedSpec = z.infer<typeof modelGradedSpecSchema>;
declare const templateStrings: {
    readonly battle: string;
    readonly closed_q_a: string;
    readonly factuality: string;
    readonly humor: string;
    readonly possible: string;
    readonly security: string;
    readonly sql: string;
    readonly summary: string;
    readonly translation: string;
};
declare const templates: Record<keyof typeof templateStrings, ModelGradedSpec>;

interface ScorerWithPartial<Output, Extra> extends Scorer<Output, Extra> {
    partial: <T extends keyof Extra>(args: {
        [K in T]: Extra[K];
    }) => Scorer<Output, Omit<Extra, T> & Partial<Pick<Extra, T>>>;
}
declare function makePartial<Output, Extra>(fn: Scorer<Output, Extra>, name?: string): ScorerWithPartial<Output, Extra>;

type LLMArgs = {
    maxTokens?: number;
    temperature?: number;
} & OpenAIAuth;
declare const DEFAULT_MODEL = "gpt-4o";
declare function buildClassificationTools(useCoT: boolean, choiceStrings: string[]): ChatCompletionTool[];
type OpenAIClassifierArgs<RenderArgs> = {
    name: string;
    model: string;
    messages: ChatCompletionMessageParam[];
    choiceScores: Record<string, number>;
    classificationTools: ChatCompletionTool[];
    cache?: ChatCache;
} & LLMArgs & RenderArgs;
declare function OpenAIClassifier<RenderArgs, Output>(args: ScorerArgs<Output, OpenAIClassifierArgs<RenderArgs>>): Promise<Score>;
type LLMClassifierArgs<RenderArgs> = {
    model?: string;
    useCoT?: boolean;
} & LLMArgs & RenderArgs;
declare function LLMClassifierFromTemplate<RenderArgs>({ name, promptTemplate, choiceScores, model, useCoT: useCoTArg, temperature, }: {
    name: string;
    promptTemplate: string;
    choiceScores: Record<string, number>;
    model?: string;
    useCoT?: boolean;
    temperature?: number;
}): Scorer<string, LLMClassifierArgs<RenderArgs>>;
declare function LLMClassifierFromSpec<RenderArgs>(name: string, spec: ModelGradedSpec): Scorer<any, LLMClassifierArgs<RenderArgs>>;
declare function LLMClassifierFromSpecFile<RenderArgs>(name: string, templateName: keyof typeof templates): Scorer<any, LLMClassifierArgs<RenderArgs>>;
/**
 * Test whether an output _better_ performs the `instructions` than the original
 * (expected) value.
 */
declare const Battle: ScorerWithPartial<string, LLMClassifierArgs<{
    instructions: string;
}>>;
/**
 * Test whether an output answers the `input` using knowledge built into the model.
 * You can specify `criteria` to further constrain the answer.
 */
declare const ClosedQA: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
    criteria: any;
}>>;
/**
 * Test whether an output is funny.
 */
declare const Humor: ScorerWithPartial<string, LLMClassifierArgs<{}>>;
/**
 * Test whether an output is factual, compared to an original (`expected`) value.
 */
declare const Factuality: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
    output: string;
    expected?: string;
}>>;
/**
 * Test whether an output is a possible solution to the challenge posed in the input.
 */
declare const Possible: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is malicious.
 */
declare const Security: ScorerWithPartial<string, LLMClassifierArgs<{}>>;
/**
 * Test whether a SQL query is semantically the same as a reference (output) query.
 */
declare const Sql: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is a better summary of the `input` than the original (`expected`) value.
 */
declare const Summary: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an `output` is as good of a translation of the `input` in the specified `language`
 * as an expert (`expected`) value.
 */
declare const Translation: ScorerWithPartial<string, LLMClassifierArgs<{
    language: string;
    input: string;
}>>;

/**
 * A simple scorer that uses the Levenshtein distance to compare two strings.
 */
declare const Levenshtein: ScorerWithPartial<string, {}>;
declare const LevenshteinScorer: ScorerWithPartial<string, {}>;
/**
 * A scorer that uses cosine similarity to compare two strings.
 *
 * @param args
 * @param args.prefix A prefix to prepend to the prompt. This is useful for specifying the domain of the inputs.
 * @param args.model The model to use for the embedding distance. Defaults to "text-embedding-ada-002".
 * @param args.expectedMin The minimum expected score. Defaults to 0.7. Values below this will be scored as 0, and
 * values between this and 1 will be scaled linearly.
 * @returns A score between 0 and 1, where 1 is a perfect match.
 */
declare const EmbeddingSimilarity: ScorerWithPartial<string, {
    prefix?: string;
    expectedMin?: number;
    model?: string;
} & OpenAIAuth>;

/**
 * A scorer that semantically evaluates the overlap between two lists of strings. It works by
 * computing the pairwise similarity between each element of the output and the expected value,
 * and then using Linear Sum Assignment to find the best matching pairs.
 */
declare const ListContains: ScorerWithPartial<string[], {
    pairwiseScorer?: Scorer<string, {}>;
    allowExtraEntities?: boolean;
}>;

/**
 * A scorer that uses OpenAI's moderation API to determine if AI response contains ANY flagged content.
 *
 * @param args
 * @param args.threshold Optional. Threshold to use to determine whether content has exceeded threshold. By
 * default, it uses OpenAI's default. (Using `flagged` from the response payload.)
 * @param args.categories Optional. Specific categories to look for. If not set, all categories will
 * be considered.
 * @returns A score between 0 and 1, where 1 means content passed all moderation checks.
 */
declare const Moderation: ScorerWithPartial<string, {
    threshold?: number;
} & OpenAIAuth>;

/**
 * A simple scorer that compares numbers by normalizing their difference.
 */
declare const NumericDiff: ScorerWithPartial<number, {}>;

/**
 * A simple scorer that compares JSON objects, using a customizable comparison method for strings
 * (defaults to Levenshtein) and numbers (defaults to NumericDiff).
 */
declare const JSONDiff: ScorerWithPartial<any, {
    stringScorer?: Scorer<string, object>;
    numberScorer?: Scorer<number, object>;
    preserveStrings?: boolean;
}>;
/**
 * A binary scorer that evaluates the validity of JSON output, optionally validating against a
 * JSON Schema definition (see https://json-schema.org/learn/getting-started-step-by-step#create).
 */
declare const ValidJSON: ScorerWithPartial<any, {
    schema?: any;
}>;

type RagasArgs = {
    input?: string;
    context?: string | string[];
    model?: string;
} & LLMArgs;
interface RagasEmbeddingModelArgs extends Record<string, unknown> {
    /**
      @default
      If not provided, the default model of {@link EmbeddingSimilarity} is used.
    */
    embeddingModel?: string;
}
/**
 * Estimates context recall by estimating TP and FN using annotated answer and
 * retrieved context.
 */
declare const ContextEntityRecall: ScorerWithPartial<string, RagasArgs & {
    pairwiseScorer?: Scorer<string, object>;
}>;
declare const ContextRelevancy: ScorerWithPartial<string, RagasArgs>;
declare const ContextRecall: ScorerWithPartial<string, RagasArgs>;
declare const ContextPrecision: ScorerWithPartial<string, RagasArgs>;
/**
 * Measures factual consistency of the generated answer with the given context.
 */
declare const Faithfulness: ScorerWithPartial<string, RagasArgs>;
/**
 * Scores the relevancy of the generated answer to the given question.
 * Answers with incomplete, redundant or unnecessary information are penalized.
 */
declare const AnswerRelevancy: ScorerWithPartial<string, RagasArgs & {
    strictness?: number;
} & RagasEmbeddingModelArgs>;
/**
 * Scores the semantic similarity between the generated answer and ground truth.
 */
declare const AnswerSimilarity: ScorerWithPartial<string, RagasArgs>;
/**
 * Measures answer correctness compared to ground truth using a weighted
 * average of factuality and semantic similarity.
 */
declare const AnswerCorrectness: ScorerWithPartial<string, RagasArgs & {
    factualityWeight?: number;
    answerSimilarityWeight?: number;
    answerSimilarity?: Scorer<string, object>;
}>;

/**
 * A simple scorer that tests whether two values are equal. If the value is an object or array,
 * it will be JSON-serialized and the strings compared for equality.
 */
declare const ExactMatch: ScorerWithPartial<unknown, {}>;
declare function normalizeValue(value: unknown, maybeObject: boolean): string;

interface AutoevalMethod {
    method: ScorerWithPartial<any, any>;
    description: string;
    template?: ModelGradedSpec;
    requiresExtraParams?: boolean;
}
declare const Evaluators: {
    label: string;
    methods: AutoevalMethod[];
}[];

export { AnswerCorrectness, AnswerRelevancy, AnswerSimilarity, Battle, ClosedQA, ContextEntityRecall, ContextPrecision, ContextRecall, ContextRelevancy, DEFAULT_MODEL, EmbeddingSimilarity, Evaluators, ExactMatch, Factuality, Faithfulness, Humor, JSONDiff, type LLMArgs, type LLMClassifierArgs, LLMClassifierFromSpec, LLMClassifierFromSpecFile, LLMClassifierFromTemplate, Levenshtein, LevenshteinScorer, ListContains, type ModelGradedSpec, Moderation, NumericDiff, OpenAIClassifier, type OpenAIClassifierArgs, Possible, type Score, type Scorer, type ScorerArgs, type ScorerWithPartial, Security, Sql, Summary, Translation, ValidJSON, buildClassificationTools, init, makePartial, modelGradedSpecSchema, normalizeValue, templates };
