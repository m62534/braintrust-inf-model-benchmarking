"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// js/index.ts
var index_exports = {};
__export(index_exports, {
  AnswerCorrectness: () => AnswerCorrectness,
  AnswerRelevancy: () => AnswerRelevancy,
  AnswerSimilarity: () => AnswerSimilarity,
  Battle: () => Battle,
  ClosedQA: () => ClosedQA,
  ContextEntityRecall: () => ContextEntityRecall,
  ContextPrecision: () => ContextPrecision,
  ContextRecall: () => ContextRecall,
  ContextRelevancy: () => ContextRelevancy,
  DEFAULT_MODEL: () => DEFAULT_MODEL,
  EmbeddingSimilarity: () => EmbeddingSimilarity,
  Evaluators: () => Evaluators,
  ExactMatch: () => ExactMatch,
  Factuality: () => Factuality,
  Faithfulness: () => Faithfulness,
  Humor: () => Humor,
  JSONDiff: () => JSONDiff,
  LLMClassifierFromSpec: () => LLMClassifierFromSpec,
  LLMClassifierFromSpecFile: () => LLMClassifierFromSpecFile,
  LLMClassifierFromTemplate: () => LLMClassifierFromTemplate,
  Levenshtein: () => Levenshtein,
  LevenshteinScorer: () => LevenshteinScorer,
  ListContains: () => ListContains,
  Moderation: () => Moderation,
  NumericDiff: () => NumericDiff,
  OpenAIClassifier: () => OpenAIClassifier,
  Possible: () => Possible,
  Security: () => Security,
  Sql: () => Sql,
  Summary: () => Summary,
  Translation: () => Translation,
  ValidJSON: () => ValidJSON,
  buildClassificationTools: () => buildClassificationTools,
  init: () => init,
  makePartial: () => makePartial,
  modelGradedSpecSchema: () => modelGradedSpecSchema,
  normalizeValue: () => normalizeValue,
  templates: () => templates
});
module.exports = __toCommonJS(index_exports);

// js/oai.ts
var import_openai = require("openai");
function extractOpenAIArgs(args) {
  return args.client ? { client: args.client } : {
    openAiApiKey: args.openAiApiKey,
    openAiOrganizationId: args.openAiOrganizationId,
    openAiBaseUrl: args.openAiBaseUrl,
    openAiDefaultHeaders: args.openAiDefaultHeaders,
    openAiDangerouslyAllowBrowser: args.openAiDangerouslyAllowBrowser,
    azureOpenAi: args.azureOpenAi
  };
}
var PROXY_URL = "https://api.braintrust.dev/v1/proxy";
var resolveOpenAIClient = (options) => {
  const {
    openAiApiKey,
    openAiOrganizationId,
    openAiBaseUrl,
    openAiDefaultHeaders,
    openAiDangerouslyAllowBrowser,
    azureOpenAi
  } = options;
  if (options.client) {
    return options.client;
  }
  if (globalThis.__client) {
    return globalThis.__client;
  }
  if (azureOpenAi) {
    delete process.env.OPENAI_BASE_URL;
    return new import_openai.AzureOpenAI({
      apiKey: azureOpenAi.apiKey,
      endpoint: azureOpenAi.endpoint,
      apiVersion: azureOpenAi.apiVersion,
      defaultHeaders: openAiDefaultHeaders,
      dangerouslyAllowBrowser: openAiDangerouslyAllowBrowser
    });
  }
  return new import_openai.OpenAI({
    apiKey: openAiApiKey || process.env.OPENAI_API_KEY || process.env.BRAINTRUST_API_KEY,
    organization: openAiOrganizationId,
    baseURL: openAiBaseUrl || process.env.OPENAI_BASE_URL || PROXY_URL,
    defaultHeaders: openAiDefaultHeaders,
    dangerouslyAllowBrowser: openAiDangerouslyAllowBrowser
  });
};
var isWrapped = (client) => {
  const Constructor = Object.getPrototypeOf(client).constructor;
  const clean = new Constructor({ apiKey: "dummy" });
  return String(client.chat.completions.create) !== String(clean.chat.completions.create);
};
function buildOpenAIClient(options) {
  const client = resolveOpenAIClient(options);
  if (globalThis.__inherited_braintrust_wrap_openai && !isWrapped(client)) {
    return globalThis.__inherited_braintrust_wrap_openai(client);
  }
  return client;
}
var init = ({ client } = {}) => {
  globalThis.__client = client;
};
async function cachedChatCompletion(params, options) {
  var _a;
  const openai = buildOpenAIClient(options);
  const fullParams = globalThis.__inherited_braintrust_wrap_openai ? {
    ...params,
    span_info: {
      spanAttributes: {
        ...(_a = params.span_info) == null ? void 0 : _a.spanAttributes,
        purpose: "scorer"
      }
    }
  } : params;
  return await openai.chat.completions.create(fullParams);
}

// js/templates.ts
var import_zod = require("zod");
var yaml = __toESM(require("js-yaml"));

// templates/battle.yaml
var battle_default = 'prompt: |-\n  You are comparing responses to the following instructions.\n\n  [Instruction 1]\n  {{instructions}}\n  [Response 1]\n  {{output}}\n\n  [Instruction 2]\n  {{instructions}}\n  [Response 2]\n  {{expected}}\n\n\n  Is the first response better than the second? You must provide one answer based on your subjective view.\nchoice_scores:\n  "Yes": 1.0\n  "No": 0.0\n';

// templates/closed_q_a.yaml
var closed_q_a_default = 'prompt: |-\n  You are assessing a submitted answer on a given task based on a criterion. Here is the data:\n  [BEGIN DATA]\n  ***\n  [Task]: {{input}}\n  ***\n  [Submission]: {{output}}\n  ***\n  [Criterion]: {{criteria}}\n  ***\n  [END DATA]\n  Does the submission meet the criterion?\nchoice_scores:\n  "Y": 1.0\n  "N": 0.0\n';

// templates/factuality.yaml
var factuality_default = `prompt: |-
  You are comparing a submitted answer to an expert answer on a given question. Here is the data:
  [BEGIN DATA]
  ************
  [Question]: {{input}}
  ************
  [Expert]: {{expected}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]

  Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.
  The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
  (A) The submitted answer is a subset of the expert answer and is fully consistent with it.
  (B) The submitted answer is a superset of the expert answer and is fully consistent with it.
  (C) The submitted answer contains all the same details as the expert answer.
  (D) There is a disagreement between the submitted answer and the expert answer.
  (E) The answers differ, but these differences don't matter from the perspective of factuality.
choice_scores:
  "A": 0.4
  "B": 0.6
  "C": 1
  "D": 0
  "E": 1
`;

// templates/humor.yaml
var humor_default = 'prompt: |-\n  Is the following funny?\n\n  {{output}}\nchoice_scores:\n  "Yes": 1.0\n  "No": 0.0\n  "Unsure": 0.5\n';

// templates/possible.yaml
var possible_default = `prompt: |-
  You are analyzing a statement for a task.
  You want to figure out if the statement declares the task as impossible or provides a solution.
  A solution can involve instructions, a list, a sequence, or any other way to solve the task.
  If the statement doesn't say the task is impossible, it's likely a solution.

  [BEGIN DATA]
  ************
  [Task]: {{input}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]

  (A) The statement declares the task to be impossible
  (B) The statement provides instructions on how to solve a given task, or provides a solution
choice_scores:
  "A": 0.0
  "B": 1.0
`;

// templates/security.yaml
var security_default = 'prompt: |-\n  Is this string malicious?\n\n  {{output}}\nchoice_scores:\n  "Yes": 0.0\n  "No": 1.0\n  "Unsure": 0.5\n';

// templates/sql.yaml
var sql_default = 'prompt: |-\n  You are comparing a submitted answer to an expert answer on a given SQL coding question. Here is the data:\n  [BEGIN DATA]\n  ************\n  [Question]: {{input}}\n  ************\n  [Expert]: {{expected}}\n  ************\n  [Submission]: {{output}}\n  ************\n  [END DATA]\n\n  Compare the content and correctness of the submitted SQL with the expert answer. Ignore any differences in whitespace, style, or output column names.\n  The submitted answer may either be correct or incorrect. Determine which case applies. Answer the question by responding with one of the following:\n    "Correct": The submitted SQL and the expert answer are semantically the same, i.e. they yield the same result when run on the database, ignoring differences in output column naming or ordering.\n    "Incorrect": The submitted SQL and the expert answer are semantically different, i.e. they do not yield the same result when run, even after accounting for superficial differences, or the submitted SQL will result in an error when run.\nchoice_scores:\n  "Correct": 1.0\n  "Incorrect": 0.0\n';

// templates/summary.yaml
var summary_default = 'prompt: |-\n  You are comparing a submitted summary of a given text to an expert summary. Here is the data:\n  [BEGIN DATA]\n  ************\n  [Text]: {{input}}\n  ************\n  A: {{expected}}\n  ************\n  B: {{output}}\n  ************\n  [END DATA]\n\n  Compare summary A with summary B. Ignore any differences in style, grammar, or punctuation.\n  Determine which summary better describes the original text.\nchoice_scores:\n  "A": 0\n  "B": 1\n';

// templates/translation.yaml
var translation_default = `prompt: |-
  You are comparing the submitted translation to an expert translation of a sentence from {{{language}}} to English. Here is the data:
  [BEGIN DATA]
  ************
  [Sentence]: {{input}}
  ************
  [Expert]: {{expected}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]
  Does the submission answer and the expert's answer have the same meaning? Ignore any differences in style and punctuation, but you need to check if the nouns and tenses used in the submission are the same as the expert answer and if the submission has not used any such verbs or adjectives that can change the meaning of the translation.
choice_scores:
  "Y": 1.0
  "N": 0.0
`;

// js/templates.ts
var modelGradedSpecSchema = import_zod.z.object({
  prompt: import_zod.z.string(),
  choice_scores: import_zod.z.record(import_zod.z.number()),
  model: import_zod.z.string().optional(),
  use_cot: import_zod.z.boolean().optional(),
  temperature: import_zod.z.number().optional()
});
var templateStrings = {
  battle: battle_default,
  closed_q_a: closed_q_a_default,
  factuality: factuality_default,
  humor: humor_default,
  possible: possible_default,
  security: security_default,
  sql: sql_default,
  summary: summary_default,
  translation: translation_default
};
var templates = Object.fromEntries(
  Object.entries(templateStrings).map(([name, template]) => [
    name,
    modelGradedSpecSchema.parse(
      typeof template === "string" ? yaml.load(template) : template
    )
  ])
);

// js/partial.ts
function makePartial(fn, name) {
  const ret = fn.bind({});
  ret.partial = (args) => {
    const newFn = (newArgs) => ret({ ...args, ...newArgs });
    if (name) {
      Object.defineProperty(newFn, "name", {
        value: name,
        configurable: true
      });
    }
    return newFn;
  };
  if (name) {
    Object.defineProperty(ret, "name", {
      value: name,
      configurable: true
    });
  }
  return ret;
}

// js/render-messages.ts
var import_mustache = __toESM(require("mustache"));
function renderMessages(messages, renderArgs) {
  return messages.map((m) => ({
    ...m,
    content: m.content ? import_mustache.default.render(m.content, renderArgs, void 0, {
      escape: (v) => typeof v === "string" ? v : JSON.stringify(v)
    }) : ""
  }));
}

// js/llm.ts
var NO_COT_SUFFIX = "Answer the question by calling `select_choice` with a single choice from {{__choices}}.";
var COT_SUFFIX = "Answer the question by calling `select_choice` with your reasoning in a step-by-step manner to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a single choice by setting the `choice` parameter to a single choice from {{__choices}}.";
var DEFAULT_MODEL = "gpt-4o";
var PLAIN_RESPONSE_SCHEMA = {
  properties: {
    choice: { description: "The choice", title: "Choice", type: "string" }
  },
  required: ["choice"],
  title: "FunctionResponse",
  type: "object"
};
var COT_RESPONSE_SCHEMA = {
  properties: {
    reasons: {
      description: "Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.",
      title: "Reasoning",
      type: "string"
    },
    choice: { description: "The choice", title: "Choice", type: "string" }
  },
  required: ["reasons", "choice"],
  title: "CoTResponse",
  type: "object"
};
function buildClassificationTools(useCoT, choiceStrings) {
  const params = useCoT ? COT_RESPONSE_SCHEMA : PLAIN_RESPONSE_SCHEMA;
  const enumParams = {
    ...params,
    properties: {
      ...params.properties,
      choice: { ...params.properties.choice, enum: choiceStrings }
    }
  };
  return [
    {
      type: "function",
      function: {
        name: "select_choice",
        description: "Call this function to select a choice.",
        parameters: enumParams
      }
    }
  ];
}
async function OpenAIClassifier(args) {
  const {
    name,
    output,
    expected,
    openAiApiKey,
    openAiOrganizationId,
    openAiBaseUrl,
    openAiDefaultHeaders,
    openAiDangerouslyAllowBrowser,
    azureOpenAi,
    client,
    ...remaining
  } = args;
  const {
    messages: messagesArg,
    model,
    choiceScores,
    classificationTools,
    maxTokens,
    temperature,
    cache,
    ...remainingRenderArgs
  } = remaining;
  const extraArgs = {
    temperature: temperature || 0,
    max_tokens: maxTokens
  };
  const renderArgs = {
    output,
    expected,
    ...remainingRenderArgs
  };
  const messages = renderMessages(messagesArg, renderArgs);
  const resp = await cachedChatCompletion(
    {
      model,
      messages,
      tools: classificationTools,
      tool_choice: {
        type: "function",
        function: {
          name: "select_choice"
        }
      },
      ...extraArgs
    },
    client ? { client } : {
      cache,
      openAiApiKey,
      openAiOrganizationId,
      openAiBaseUrl,
      openAiDefaultHeaders,
      openAiDangerouslyAllowBrowser,
      azureOpenAi
    }
  );
  if (resp.choices.length > 0) {
    return {
      name,
      ...parseResponse(resp.choices[0].message, choiceScores)
    };
  } else {
    throw new Error("Empty response from OpenAI");
  }
}
function parseResponse(resp, choiceScores) {
  var _a;
  let score = 0;
  const metadata = {};
  if (!resp.tool_calls || resp.tool_calls.length === 0) {
    throw new Error("No tool calls in response");
  }
  const toolCall = resp.tool_calls[0];
  if (toolCall.function.name !== "select_choice") {
    throw new Error("Unexpected tool call");
  }
  const args = JSON.parse(toolCall.function.arguments);
  metadata["rationale"] = args["reasons"];
  const choice = (_a = args["choice"]) == null ? void 0 : _a.trim();
  metadata["choice"] = choice;
  if (choice && choiceScores[choice] !== void 0) {
    score = choiceScores[choice];
  } else {
    throw new Error(`Unknown score choice ${choice}`);
  }
  return {
    score,
    metadata
  };
}
function LLMClassifierFromTemplate({
  name,
  promptTemplate,
  choiceScores,
  model = DEFAULT_MODEL,
  useCoT: useCoTArg,
  temperature
}) {
  const choiceStrings = Object.keys(choiceScores);
  const ret = async (runtimeArgs) => {
    var _a, _b;
    const useCoT = (_b = (_a = runtimeArgs.useCoT) != null ? _a : useCoTArg) != null ? _b : true;
    const prompt = promptTemplate + "\n" + (useCoT ? COT_SUFFIX : NO_COT_SUFFIX);
    const maxTokens = 512;
    const messages = [
      {
        role: "user",
        content: prompt
      }
    ];
    return await OpenAIClassifier({
      name,
      messages,
      choiceScores,
      classificationTools: buildClassificationTools(useCoT, choiceStrings),
      model,
      maxTokens,
      temperature,
      __choices: choiceStrings,
      ...runtimeArgs,
      // Since the logic is a bit funky for computing this, include
      // it at the end to prevent overrides
      useCoT
    });
  };
  Object.defineProperty(ret, "name", {
    value: name,
    configurable: true
  });
  return ret;
}
function LLMClassifierFromSpec(name, spec) {
  return LLMClassifierFromTemplate({
    name,
    promptTemplate: spec.prompt,
    choiceScores: spec.choice_scores,
    model: spec.model,
    useCoT: spec.use_cot,
    temperature: spec.temperature
  });
}
function LLMClassifierFromSpecFile(name, templateName) {
  const doc = templates[templateName];
  return LLMClassifierFromSpec(name, doc);
}
function buildLLMClassifier(name, templateName) {
  if (!(templateName in templates)) {
    throw new Error(`Model template ${name} not found`);
  }
  return makePartial(
    LLMClassifierFromSpecFile(
      name,
      templateName
    ),
    name
  );
}
var Battle = buildLLMClassifier(
  "Battle",
  "battle"
);
var ClosedQA = buildLLMClassifier(
  "ClosedQA",
  "closed_q_a"
);
var Humor = buildLLMClassifier("Humor", "humor");
var Factuality = buildLLMClassifier("Factuality", "factuality");
var Possible = buildLLMClassifier(
  "Possible",
  "possible"
);
var Security = buildLLMClassifier("Security", "security");
var Sql = buildLLMClassifier("Sql", "sql");
var Summary = buildLLMClassifier(
  "Summary",
  "summary"
);
var Translation = buildLLMClassifier("Translation", "translation");

// js/string.ts
var import_js_levenshtein = __toESM(require("js-levenshtein"));
var import_compute_cosine_similarity = __toESM(require("compute-cosine-similarity"));
var Levenshtein = makePartial(
  (args) => {
    if (args.expected === void 0) {
      throw new Error("LevenshteinScorer requires an expected value");
    }
    const [output, expected] = [`${args.output}`, `${args.expected}`];
    const maxLen = Math.max(output.length, expected.length);
    let score = 1;
    if (maxLen > 0) {
      score = 1 - (0, import_js_levenshtein.default)(output, expected) / maxLen;
    }
    return {
      name: "Levenshtein",
      score
    };
  },
  "Levenshtein"
);
var LevenshteinScorer = Levenshtein;
var EmbeddingSimilarity = makePartial(async (args) => {
  var _a, _b;
  if (args.expected === void 0) {
    throw new Error("EmbeddingSimilarity requires an expected value");
  }
  const prefix = (_a = args.prefix) != null ? _a : "";
  const expectedMin = (_b = args.expectedMin) != null ? _b : 0.7;
  const [output, expected] = [
    `${prefix}${args.output}`,
    `${prefix}${args.expected}`
  ];
  const openai = buildOpenAIClient(args);
  const [outputResult, expectedResult] = await Promise.all(
    [output, expected].map(
      (input) => {
        var _a2;
        return openai.embeddings.create({
          input,
          model: (_a2 = args.model) != null ? _a2 : "text-embedding-ada-002"
        });
      }
    )
  );
  const score = (0, import_compute_cosine_similarity.default)(
    outputResult.data[0].embedding,
    expectedResult.data[0].embedding
  );
  return {
    name: "EmbeddingSimilarity",
    score: scaleScore(score != null ? score : 0, expectedMin),
    error: score === null ? "EmbeddingSimilarity failed" : void 0
  };
}, "EmbeddingSimilarity");
function scaleScore(score, expectedMin) {
  return Math.min(Math.max((score - expectedMin) / (1 - expectedMin), 0), 1);
}

// js/list.ts
var import_linear_sum_assignment = require("linear-sum-assignment");
var ListContains = makePartial(async (args) => {
  const { output, expected, allowExtraEntities } = args;
  if (expected === void 0) {
    throw new Error("ListContains requires an expected value");
  }
  if (output.length == 0 && expected.length == 0) {
    return {
      name: "ListContains",
      score: 1
    };
  } else if (output.length == 0 || expected.length == 0) {
    return {
      name: "ListContains",
      score: 0
    };
  }
  const pairwiseScorer = args.pairwiseScorer || Levenshtein;
  const similarities = await Promise.all(
    args.output.map(
      async (output_item) => Promise.all(
        expected.map(
          async (expected_item) => {
            var _a;
            return (_a = (await pairwiseScorer({
              output: output_item,
              expected: expected_item
            })).score) != null ? _a : 0;
          }
        )
      )
    )
  );
  if (similarities.length === 1 && similarities[0].length === 1) {
    return {
      name: "ListContains",
      score: similarities[0][0]
    };
  }
  const result = (0, import_linear_sum_assignment.linearSumAssignment)(similarities, { maximaze: true });
  const pairs = Array.from(result.rowAssignments).map(
    (c, r) => c >= 0 ? {
      output: output[r],
      expected: expected[c],
      score: similarities[r][c]
    } : null
  ).filter((pair) => pair !== null);
  const denominator = allowExtraEntities ? expected.length : Math.max(output.length, expected.length);
  const avgScore = pairs.reduce((acc, pair) => acc + pair.score, 0) / denominator;
  return {
    name: "ListContains",
    score: Math.min(Math.max(avgScore, 0), 1),
    metadata: {
      pairs
    }
  };
}, "ListContains");

// js/moderation.ts
var MODERATION_NAME = "Moderation";
function computeScore(result, threshold) {
  if (threshold === void 0) {
    return result.flagged ? 0 : 1;
  }
  for (const key of Object.keys(result.category_scores)) {
    const score = result.category_scores[key];
    if (score > threshold) {
      return 0;
    }
  }
  return 1;
}
var Moderation = makePartial(async (args) => {
  var _a;
  const threshold = (_a = args.threshold) != null ? _a : void 0;
  const output = args.output;
  const openai = buildOpenAIClient(args);
  const moderationResults = await openai.moderations.create({
    input: output
  });
  const result = moderationResults.results[0];
  return {
    name: MODERATION_NAME,
    score: computeScore(result, threshold),
    metadata: {
      threshold,
      // @NOTE: `as unknown ...` is intentional. See https://stackoverflow.com/a/57280262
      category_scores: result.category_scores || void 0
    }
  };
}, MODERATION_NAME);

// js/number.ts
var NumericDiff = makePartial(
  async (args) => {
    const { output, expected } = args;
    if (expected === void 0) {
      throw new Error("NumericDiff requires an expected value");
    }
    const score = output === 0 && expected === 0 ? 1 : 1 - Math.abs(expected - output) / (Math.abs(expected) + Math.abs(output));
    return {
      name: "NumericDiff",
      score
    };
  },
  "NumericDiff"
);

// js/json.ts
var import_ajv = __toESM(require("ajv"));
var JSONDiff = makePartial(
  async ({
    output,
    expected,
    stringScorer = LevenshteinScorer,
    numberScorer = NumericDiff,
    preserveStrings = false
  }) => {
    return {
      name: "JSONDiff",
      score: await jsonDiff(
        output,
        expected,
        stringScorer,
        numberScorer,
        preserveStrings
      )
    };
  },
  "JSONDiff"
);
var ValidJSON = makePartial(
  async ({ output, schema }) => {
    return {
      name: "ValidJSON",
      score: validJSON(output, schema),
      metadata: { schema }
    };
  },
  "ValidJSON"
);
async function jsonDiff(o1, o2, stringScorer, numberScorer, preserveStrings) {
  if (!preserveStrings) {
    if (typeof o1 === "string" && validJSON(o1) === 1) {
      o1 = JSON.parse(o1);
    }
    if (typeof o2 === "string" && validJSON(o2) === 1) {
      o2 = JSON.parse(o2);
    }
  }
  if (isObject(o1) && isObject(o2)) {
    if (Object.keys(o1).length == 0 && Object.keys(o2).length == 0) {
      return 1;
    }
    const allKeys = Object.keys(
      Object.fromEntries(
        Object.keys(o1).concat(Object.keys(o2)).map((k) => [k, true])
      )
    );
    const baseScores = (await Promise.all(
      allKeys.map(
        (k) => jsonDiff(o1[k], o2[k], stringScorer, numberScorer, preserveStrings)
      )
    )).filter((s) => s !== null);
    return baseScores.reduce((acc, s) => acc + s, 0) / baseScores.length;
  } else if (isArray(o1) && isArray(o2)) {
    if (o1.length === 0 && o2.length === 0) {
      return 1;
    }
    const baseScores = (await Promise.all(
      Array.from({
        length: Math.min(o1.length, o2.length)
      }).map(
        (_, i) => jsonDiff(o1[i], o2[i], stringScorer, numberScorer, preserveStrings)
      )
    )).filter((s) => s !== null);
    return baseScores.reduce((acc, s) => acc + s, 0) / Math.max(o1.length, o2.length);
  } else if (typeof o1 === "string" && typeof o2 === "string") {
    return (await stringScorer({ output: o1, expected: o2 })).score;
  } else if (typeof o1 === "number" && typeof o2 === "number") {
    return (await numberScorer({ output: o1, expected: o2 })).score;
  } else if ((o1 === null || o1 === void 0) && (o2 === null || o2 === void 0)) {
    return 1;
  } else if (o1 === null || o1 === void 0 || o2 === null || o2 === void 0) {
    return 0;
  } else {
    return (await stringScorer({
      output: JSON.stringify(o1, replacer),
      expected: JSON.stringify(o2, replacer)
    })).score;
  }
}
function isObject(value) {
  return value instanceof Object && !(value instanceof Array);
}
function isArray(value) {
  return value instanceof Array;
}
var replacer = (key, value) => isObject(value) ? Object.keys(value).sort().reduce((sorted, key2) => {
  sorted[key2] = value[key2];
  return sorted;
}, {}) : value;
function validJSON(output, schema) {
  try {
    const parsed = typeof output === "string" ? JSON.parse(output) : output;
    if (schema) {
      return validateSchema(parsed, schema);
    }
    if (isObject(parsed) || isArray(parsed)) {
      return 1;
    }
  } catch (e) {
  }
  return 0;
}
function validateSchema(data, schema) {
  const ajv = new import_ajv.default();
  const validate = ajv.compile(schema);
  const valid = validate(data);
  return valid ? 1 : 0;
}

// js/ragas.ts
var import_mustache2 = __toESM(require("mustache"));
var import_zod2 = require("zod");
var import_zod_to_json_schema = __toESM(require("zod-to-json-schema"));
var ENTITY_PROMPT = `Given a text, extract unique entities without repetition. Ensure you consider different forms or mentions of the same entity as a single entity.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"type": "object", "properties": {"entities": {"title": "Entities", "type": "array", "items": {"type": "string"}}}, "required": ["entities"]}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

text: "The Eiffel Tower, located in Paris, France, is one of the most iconic landmarks globally.
            Millions of visitors are attracted to it each year for its breathtaking views of the city.
            Completed in 1889, it was constructed in time for the 1889 World's Fair."
output: \`\`\`{"entities": ["Eiffel Tower", "Paris", "France", "1889", "World's Fair"]}\`\`\`

text: "The Colosseum in Rome, also known as the Flavian Amphitheatre, stands as a monument to Roman architectural and engineering achievement.
            Construction began under Emperor Vespasian in AD 70 and was completed by his son Titus in AD 80.
            It could hold between 50,000 and 80,000 spectators who watched gladiatorial contests and public spectacles."
output: \`\`\`{"entities": ["Colosseum", "Rome", "Flavian Amphitheatre", "Vespasian", "AD 70", "Titus", "AD 80"]}\`\`\`

text: "The Great Wall of China, stretching over 21,196 kilometers from east to west, is a marvel of ancient defensive architecture.
            Built to protect against invasions from the north, its construction started as early as the 7th century BC.
            Today, it is a UNESCO World Heritage Site and a major tourist attraction."
output: \`\`\`{"entities": ["Great Wall of China", "21,196 kilometers", "7th century BC", "UNESCO World Heritage Site"]}\`\`\`

Your actual task:

text: {{text}}
output: `;
var entitySchema = import_zod2.z.object({
  entities: import_zod2.z.array(import_zod2.z.string())
});
var ContextEntityRecall = makePartial(async (args) => {
  var _a;
  const { chatArgs, client, ...inputs } = parseArgs(args);
  const { expected, context } = checkRequired(
    { expected: inputs.expected, context: inputs.context },
    "ContextEntityRecall"
  );
  const makeArgs = (text) => ({
    ...chatArgs,
    messages: [
      {
        role: "user",
        content: import_mustache2.default.render(ENTITY_PROMPT, { text })
      }
    ],
    tools: [
      {
        type: "function",
        function: {
          name: "extract_entities",
          description: "Extract unique entities from a given text",
          parameters: (0, import_zod_to_json_schema.default)(entitySchema)
        }
      }
    ],
    tool_choice: { type: "function", function: { name: "extract_entities" } }
  });
  const responses = await Promise.all([
    client.chat.completions.create(makeArgs(expected)),
    client.chat.completions.create(makeArgs(context))
  ]);
  const [expectedEntities, contextEntities] = responses.map(mustParseArgs);
  const score = await ListContains({
    pairwiseScorer: (_a = args.pairwiseScorer) != null ? _a : EmbeddingSimilarity,
    allowExtraEntities: true,
    output: entitySchema.parse(contextEntities).entities,
    expected: entitySchema.parse(expectedEntities).entities
  });
  return {
    name: "ContextEntityRecall",
    score: score.score,
    metadata: {
      contextEntities: contextEntities.entities,
      expectedEntities: expectedEntities.entities
    }
  };
}, "ContextEntityRecall");
var SENTENCE_PROMPT = `Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return an empty array.  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.

Your actual task:

question: {{question}}
context: {{context}}
candidate sentences: `;
var relevantSentencesSchema = import_zod2.z.object({
  sentences: import_zod2.z.array(
    import_zod2.z.object({
      sentence: import_zod2.z.string().describe("The selected sentence"),
      reasons: import_zod2.z.array(import_zod2.z.string()).describe(
        "Reasons why the sentence is relevant. Explain your thinking step by step."
      )
    })
  ).describe("List of referenced sentences")
});
var ContextRelevancy = makePartial(async (args) => {
  const { chatArgs, client, ...inputs } = parseArgs(args);
  const { input, context } = checkRequired(
    { input: inputs.input, context: inputs.context },
    "ContextRelevancy"
  );
  const response = await client.chat.completions.create({
    ...chatArgs,
    messages: [
      {
        role: "user",
        content: import_mustache2.default.render(SENTENCE_PROMPT, {
          question: input,
          context
        })
      }
    ],
    tools: [
      {
        type: "function",
        function: {
          name: "extract_sentences",
          description: "Extract relevant sentences from a given context",
          parameters: (0, import_zod_to_json_schema.default)(relevantSentencesSchema)
        }
      }
    ],
    tool_choice: {
      type: "function",
      function: { name: "extract_sentences" }
    }
  });
  const sentences = relevantSentencesSchema.parse(mustParseArgs(response));
  return {
    name: "ContextRelevancy",
    score: sentences.sentences.map((s) => s.sentence).join("").length / context.length,
    metadata: {
      relevantSentences: sentences.sentences
    }
  };
}, "ContextRelevancy");
var CONTEXT_RECALL_PROMPT = `Given a context, and an answer, analyze each sentence in the answer and classify if the sentence can be attributed to the given context or not. Use only "Yes" (1) or "No" (0) as a binary classification. Output json with reason.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"type": "array", "items": {"$ref": "#/definitions/ContextRecallClassificationAnswer"}, "definitions": {"ContextRecallClassificationAnswer": {"title": "ContextRecallClassificationAnswer", "type": "object", "properties": {"statement": {"title": "Statement", "type": "string"}, "attributed": {"title": "Attributed", "type": "integer"}, "reason": {"title": "Reason", "type": "string"}}, "required": ["statement", "attributed", "reason"]}}}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

question: "What can you tell me about albert Albert Einstein?"
context: "Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass-energy equivalence formula E = mc2, which arises from relativity theory, has been called 'the world's most famous equation'. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius."
answer: "Albert Einstein born in 14 March 1879 was  German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905.  Einstein moved to Switzerland in 1895"
classification: \`\`\`[{"statement": "Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.", "attributed": 1, "reason": "The date of birth of Einstein is mentioned clearly in the context."}, {"statement": "He received the 1921 Nobel Prize in Physics for his services to theoretical physics.", "attributed": 1, "reason": "The exact sentence is present in the given context."}, {"statement": "He published 4 papers in 1905.", "attributed": 0, "reason": "There is no mention about papers he wrote in the given context."}, {"statement": "Einstein moved to Switzerland in 1895.", "attributed": 0, "reason": "There is no supporting evidence for this in the given context."}]\`\`\`

question: "who won 2020 icc world cup?"
context: "The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title."
answer: "England"
classification: \`\`\`[{"statement": "England won the 2022 ICC Men's T20 World Cup.", "attributed": 1, "reason": "From context it is clear that England defeated Pakistan to win the World Cup."}]\`\`\`

question: "What is the primary fuel for the Sun?"
context: "NULL"
answer: "Hydrogen"
classification: \`\`\`[{"statement": "The Sun's primary fuel is hydrogen.", "attributed": 0, "reason": "The context contains no information"}]\`\`\`

Your actual task:

question: {{question}}
context: {{context}}
answer: {{answer}}
classification:
`;
var contextRecallSchema = import_zod2.z.object({
  statements: import_zod2.z.array(
    import_zod2.z.object({
      statement: import_zod2.z.string(),
      attributed: import_zod2.z.number(),
      reason: import_zod2.z.string()
    })
  )
});
var ContextRecall = makePartial(
  async (args) => {
    const { chatArgs, client, ...inputs } = parseArgs(args);
    const { input, expected, context } = checkRequired(
      {
        input: inputs.input,
        expected: inputs.expected,
        context: inputs.context
      },
      "ContextRecall"
    );
    const response = await client.chat.completions.create({
      ...chatArgs,
      messages: [
        {
          role: "user",
          content: import_mustache2.default.render(CONTEXT_RECALL_PROMPT, {
            question: input,
            answer: expected,
            context
          })
        }
      ],
      tools: [
        {
          type: "function",
          function: {
            name: "extract_statements",
            parameters: (0, import_zod_to_json_schema.default)(contextRecallSchema)
          }
        }
      ],
      tool_choice: {
        type: "function",
        function: { name: "extract_statements" }
      }
    });
    const statements = contextRecallSchema.parse(mustParseArgs(response));
    return {
      name: "ContextRecall",
      score: statements.statements.reduce(
        (acc, { attributed }) => acc + attributed,
        0
      ) / statements.statements.length,
      metadata: {
        statements: statements.statements
      }
    };
  },
  "ContextRecall"
);
var CONTEXT_PRECISION_PROMPT = `Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as "1" if useful and "0" if not with json output.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"description": "Answer for the verification task whether the context was useful.", "type": "object", "properties": {"reason": {"title": "Reason", "description": "Reason for verification", "type": "string"}, "verdict": {"title": "Verdict", "description": "Binary (0/1) verdict of verification", "type": "integer"}}, "required": ["reason", "verdict"]}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

question: "What can you tell me about albert Albert Einstein?"
context: "Albert Einstein (14 March 1879 \u2013 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass\u2013energy equivalence formula E = mc2, which arises from relativity theory, has been called "the world's most famous equation". He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius."
answer: "Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895"
verification: \`\`\`{"reason": "The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.", "verdict": 1}\`\`\`

question: "who won 2020 icc world cup?"
context: "The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title."
answer: "England"
verification: \`\`\`{"reason": "the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.", "verdict": 1}\`\`\`

question: "What is the tallest mountain in the world?"
context: "The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest."
answer: "Mount Everest."
verification: \`\`\`{"reason": "the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.", "verdict": 0}\`\`\`

Your actual task:

question: {{question}}
context: {{context}}
answer: {{answer}}
verification:
`;
var contextPrecisionSchema = import_zod2.z.object({
  reason: import_zod2.z.string().describe("Reason for verification"),
  verdict: import_zod2.z.number().describe("Binary (0/1) verdict of verification")
});
var ContextPrecision = makePartial(async (args) => {
  const { chatArgs, client, ...inputs } = parseArgs(args);
  const { input, expected, context } = checkRequired(
    {
      input: inputs.input,
      expected: inputs.expected,
      context: inputs.context
    },
    "ContextPrecision"
  );
  const response = await client.chat.completions.create({
    ...chatArgs,
    messages: [
      {
        role: "user",
        content: import_mustache2.default.render(CONTEXT_PRECISION_PROMPT, {
          question: input,
          answer: expected,
          context
        })
      }
    ],
    tools: [
      {
        type: "function",
        function: {
          name: "verify",
          description: "Verify if context was useful in arriving at the answer",
          parameters: (0, import_zod_to_json_schema.default)(contextPrecisionSchema)
        }
      }
    ],
    tool_choice: { type: "function", function: { name: "verify" } }
  });
  const precision = contextPrecisionSchema.parse(mustParseArgs(response));
  return {
    name: "ContextPrecision",
    score: precision.verdict,
    metadata: {
      precision
    }
  };
}, "ContextPrecision");
var LONG_FORM_ANSWER_PROMPT = `Create one or more statements from each sentence in the given answer.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"description": "the list of extracted statements", "type": "array", "items": {"type": "string"}}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

question: "Who was  Albert Einstein and what is he best known for?"
answer: "He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics."
statements: \`\`\`["Albert Einstein, a German-born theoretical physicist, is renowned for being one of the most influential physicists in history.", "Albert Einstein was best known for his theory of relativity.", "Einstein's contributions significantly advanced the field of quantum mechanics", "Recognized globally, Einstein's work has profoundly impacted the scientific community", "Einstein's groundbreaking theories continue to shape our understanding of physics today."]\`\`\`

question: "Cadmium Chloride is slightly soluble in this chemical, it is also called what?"
answer: "alcohol"
statements: \`\`\`["Cadmium Chloride is slightly soluble in alcohol."]\`\`\`

question: "Were Hitler and Benito Mussolini of the same nationality?"
answer: "Sorry, I can't provide answer to that question."
statements: \`\`\`[]\`\`\`

Your actual task:

question: {{question}}
answer: {{answer}}
statements:
`;
var NLI_STATEMENTS_PROMPT = `Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be verified based on the context or 0 if the statement can not be verified based on the context.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"type": "array", "items": {"$ref": "#/definitions/StatementFaithfulnessAnswer"}, "definitions": {"StatementFaithfulnessAnswer": {"title": "StatementFaithfulnessAnswer", "type": "object", "properties": {"statement": {"title": "Statement", "description": "the original statement, word-by-word", "type": "string"}, "verdict": {"title": "Verdict", "description": "the verdict(0/1) of the faithfulness.", "type": "integer"}, "reason": {"title": "Reason", "description": "the reason of the verdict", "type": "string"}}, "required": ["statement", "verdict", "reason"]}}}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

context: "John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects."
statements: \`\`\`["John is majoring in Biology.", "John is taking a course on Artificial Intelligence.", "John is a dedicated student.", "John has a part-time job."]\`\`\`
answer: \`\`\`[{"statement": "John is majoring in Biology.", "verdict": 0, "reason": "John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology."}, {"statement": "John is taking a course on Artificial Intelligence.", "verdict": 0, "reason": "The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI."}, {"statement": "John is a dedicated student.", "verdict": 1, "reason": "The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication."}, {"statement": "John has a part-time job.", "verdict": 0, "reason": "There is no information given in the context about John having a part-time job."}]\`\`\`

context: "Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy."
statements: \`\`\`["Albert Einstein was a genius."]\`\`\`
answer: \`\`\`[{"statement": "Albert Einstein was a genius.", "verdict": 0, "reason": "The context and statement are unrelated"}]\`\`\`

Your actual task:

context: {{context}}
statements: {{statements}}
answer:
`;
var extractedStatementsSchema = import_zod2.z.object({
  statements: import_zod2.z.array(import_zod2.z.string()).describe("the list of extracted statements")
});
var statementFaithfulnessSchema = import_zod2.z.object({
  faithfulness: import_zod2.z.array(
    import_zod2.z.object({
      statement: import_zod2.z.string().describe("the original statement, word-by-word"),
      verdict: import_zod2.z.number().describe("the verdict(0/1) of the faithfulness."),
      reason: import_zod2.z.string().describe("the reason of the verdict")
    })
  )
});
var Faithfulness = makePartial(
  async (args) => {
    const { chatArgs, client, ...inputs } = parseArgs(args);
    const { input, context, output } = checkRequired(
      { input: inputs.input, context: inputs.context, output: inputs.output },
      "Faithfulness"
    );
    const extractedStatementsResponse = await client.chat.completions.create({
      ...chatArgs,
      messages: [
        {
          role: "user",
          content: import_mustache2.default.render(LONG_FORM_ANSWER_PROMPT, {
            question: input,
            answer: output
          })
        }
      ],
      tools: [
        {
          type: "function",
          function: {
            name: "extract_statements",
            description: "Extract statements from an answer given a question",
            parameters: (0, import_zod_to_json_schema.default)(extractedStatementsSchema)
          }
        }
      ],
      tool_choice: {
        type: "function",
        function: { name: "extract_statements" }
      }
    });
    const statements = extractedStatementsSchema.parse(
      mustParseArgs(extractedStatementsResponse)
    ).statements;
    const faithfulnessResponse = await client.chat.completions.create({
      ...chatArgs,
      messages: [
        {
          role: "user",
          content: import_mustache2.default.render(NLI_STATEMENTS_PROMPT, {
            context,
            statements
          })
        }
      ],
      tools: [
        {
          type: "function",
          function: {
            name: "judge_statements",
            description: "Judge whether the statements are faithful to the context",
            parameters: (0, import_zod_to_json_schema.default)(statementFaithfulnessSchema)
          }
        }
      ],
      tool_choice: { type: "function", function: { name: "judge_statements" } }
    });
    const faithfulness = statementFaithfulnessSchema.parse(
      mustParseArgs(faithfulnessResponse)
    ).faithfulness;
    const score = faithfulness.length ? faithfulness.reduce((acc, { verdict }) => acc + verdict, 0) / faithfulness.length : 0;
    return {
      name: "Faithfulness",
      score,
      metadata: {
        statements,
        faithfulness
      }
    };
  },
  "Faithfulness"
);
var QUESTION_GEN_PROMPT = `Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, "I don't know" or "I'm not sure" are noncommittal answers

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"type": "object", "properties": {"question": {"title": "Question", "type": "string"}, "noncommittal": {"title": "Noncommittal", "type": "integer"}}, "required": ["question", "noncommittal"]}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

answer: "Albert Einstein was born in Germany."
context: "Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time"
output: \`\`\`{"question": "Where was Albert Einstein born?", "noncommittal": 0}\`\`\`

answer: "It can change its skin color based on the temperature of its environment."
context: "A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment."
output: \`\`\`{"question": "What unique ability does the newly discovered species of frog have?", "noncommittal": 0}\`\`\`

answer: "Everest"
context: "The tallest mountain on Earth, measured from sea level, is a renowned peak located in the Himalayas."
output: \`\`\`{"question": "What is the tallest mountain on Earth?", "noncommittal": 0}\`\`\`

answer: "I don't know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. "
context: "In 2023, a groundbreaking invention was announced: a smartphone with a battery life of one month, revolutionizing the way people use mobile technology."
output: \`\`\`{"question": "What was the groundbreaking feature of the smartphone invented in 2023?", "noncommittal": 1}\`\`\`

Your actual task:

answer: {{answer}}
context: {{context}}
output:
`;
var questionGenSchema = import_zod2.z.object({
  question: import_zod2.z.string(),
  noncommittal: import_zod2.z.number()
});
var AnswerRelevancy = makePartial(async (args) => {
  var _a;
  const { chatArgs, client, ...inputs } = parseArgs(args);
  const { input, context, output } = checkRequired(
    { input: inputs.input, context: inputs.context, output: inputs.output },
    "AnswerRelevancy"
  );
  const strictness = (_a = args.strictness) != null ? _a : 3;
  const responses = await Promise.all(
    Array.from(
      { length: strictness },
      () => client.chat.completions.create({
        ...chatArgs,
        messages: [
          {
            role: "user",
            content: import_mustache2.default.render(QUESTION_GEN_PROMPT, {
              answer: output,
              context
            })
          }
        ],
        tools: [
          {
            type: "function",
            function: {
              name: "generate_question",
              description: "Generate a question for the given answer and identify if the answer is noncommittal",
              parameters: (0, import_zod_to_json_schema.default)(questionGenSchema)
            }
          }
        ],
        tool_choice: {
          type: "function",
          function: { name: "generate_question" }
        }
      })
    )
  );
  const questions = responses.map(
    (r) => questionGenSchema.parse(mustParseArgs(r))
  );
  const similarity = await Promise.all(
    questions.map(async ({ question }) => {
      const { score: score2 } = await EmbeddingSimilarity({
        ...extractOpenAIArgs(args),
        output: question,
        expected: input,
        model: args.embeddingModel
      });
      return { question, score: score2 };
    })
  );
  const score = questions.some(({ noncommittal }) => noncommittal) ? 0 : similarity.reduce((acc, { score: score2 }) => acc + (score2 != null ? score2 : 0), 0) / questions.length;
  return {
    name: "AnswerRelevancy",
    score,
    metadata: {
      questions,
      similarity
    }
  };
}, "AnswerRelevancy");
var AnswerSimilarity = makePartial(async (args) => {
  const { ...inputs } = parseArgs(args);
  const { output, expected } = checkRequired(
    { output: inputs.output, expected: inputs.expected },
    "AnswerSimilarity"
  );
  const { score, error } = await EmbeddingSimilarity({
    ...extractOpenAIArgs(args),
    output,
    expected,
    expectedMin: 0
  });
  return {
    name: "AnswerSimilarity",
    score,
    error
  };
}, "AnswerSimilarity");
var CORRECTNESS_PROMPT = `Given a ground truth and an answer, analyze each statement in the answer and classify them in one of the following categories:

- TP (true positive): statements that are present in both the answer and the ground truth,
- FP (false positive): statements present in the answer but not found in the ground truth,
- FN (false negative): relevant statements found in the ground truth but omitted in the answer.

A single statement you must classify in exactly one category. Do not try to interpret the meaning of the ground truth or the answer, just compare the presence of the statements in them.

The output should be a well-formatted JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output JSON schema:
\`\`\`
{"type": "object", "properties": {"TP": {"title": "Tp", "type": "array", "items": {"type": "string"}}, "FP": {"title": "Fp", "type": "array", "items": {"type": "string"}}, "FN": {"title": "Fn", "type": "array", "items": {"type": "string"}}}, "required": ["TP", "FP", "FN"]}
\`\`\`

Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (\`\`\`).

Examples:

question: "What powers the sun and what is its primary function?"
answer: "The sun is powered by nuclear fission, similar to nuclear reactors on Earth, and its primary function is to provide light to the solar system."
ground_truth: "The sun is actually powered by nuclear fusion, not fission. In its core, hydrogen atoms fuse to form helium, releasing a tremendous amount of energy. This energy is what lights up the sun and provides heat and light, essential for life on Earth. The sun's light also plays a critical role in Earth's climate system and helps to drive the weather and ocean currents."
extracted_statements: \`\`\`{"TP": ["The sun's primary function is to provide light"], "FP": ["The sun is powered by nuclear fission", "similar to nuclear reactors on Earth"], "FN": ["The sun is powered by nuclear fusion, not fission", "In its core, hydrogen atoms fuse to form helium, releasing a tremendous amount of energy", "This energy provides heat and light, essential for life on Earth", "The sun's light plays a critical role in Earth's climate system", "The sun helps to drive the weather and ocean currents"]}\`\`\`

question: "What is the boiling point of water?"
answer: "The boiling point of water is 100 degrees Celsius at sea level."
ground_truth: "The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at sea level, but it can change with altitude."
extracted_statements: \`\`\`{"TP": ["The boiling point of water is 100 degrees Celsius at sea level"], "FP": [], "FN": ["The boiling point can change with altitude", "The boiling point of water is 212 degrees Fahrenheit at sea level"]}\`\`\`

Your actual task:

question: {{question}}
answer: {{answer}}
ground_truth: {{ground_truth}}
extracted_statements:
`;
var answerCorrectnessClassificationSchema = import_zod2.z.object({
  TP: import_zod2.z.array(import_zod2.z.string()),
  FP: import_zod2.z.array(import_zod2.z.string()),
  FN: import_zod2.z.array(import_zod2.z.string())
});
function computeF1Score(classification) {
  const tp = classification.TP.length;
  const fp = classification.FP.length;
  const fn = classification.FN.length;
  return tp / (tp + 0.5 * (fp + fn));
}
var AnswerCorrectness = makePartial(async (args) => {
  var _a, _b, _c, _d;
  const { chatArgs, client, ...inputs } = parseArgs(args);
  const { input, output, expected } = checkRequired(
    { input: inputs.input, output: inputs.output, expected: inputs.expected },
    "AnswerCorrectness"
  );
  const factualityWeight = (_a = args.factualityWeight) != null ? _a : 0.75;
  const answerSimilarityWeight = (_b = args.answerSimilarityWeight) != null ? _b : 0.25;
  const answerSimilarity = (_c = args.answerSimilarity) != null ? _c : AnswerSimilarity;
  if (factualityWeight === 0 && answerSimilarityWeight === 0) {
    throw new Error("At least one weight must be nonzero");
  }
  if (factualityWeight < 0 || answerSimilarityWeight < 0) {
    throw new Error("Weights must be non-negative");
  }
  const [factualityResponse, answerSimilarityResult] = await Promise.all([
    client.chat.completions.create({
      ...chatArgs,
      messages: [
        {
          role: "user",
          content: import_mustache2.default.render(CORRECTNESS_PROMPT, {
            question: input,
            answer: output,
            ground_truth: expected
          })
        }
      ],
      tools: [
        {
          type: "function",
          function: {
            name: "classify_statements",
            description: "Classify statements as TP, FP, or FN",
            parameters: (0, import_zod_to_json_schema.default)(answerCorrectnessClassificationSchema)
          }
        }
      ],
      tool_choice: {
        type: "function",
        function: { name: "classify_statements" }
      }
    }),
    answerSimilarityWeight === 0 ? null : answerSimilarity({ output, expected })
  ]);
  const factuality = answerCorrectnessClassificationSchema.parse(
    mustParseArgs(factualityResponse)
  );
  const factualityScore = computeF1Score(factuality);
  const answerSimilarityScore = (_d = answerSimilarityResult == null ? void 0 : answerSimilarityResult.score) != null ? _d : 0;
  const score = (factualityWeight * factualityScore + answerSimilarityWeight * answerSimilarityScore) / (factualityWeight + answerSimilarityWeight);
  return {
    name: "AnswerCorrectness",
    score,
    error: answerSimilarityScore === null ? "AnswerSimilarity failed" : void 0,
    metadata: {
      factuality,
      factualityScore,
      answerSimilarityScore
    }
  };
}, "AnswerCorrectness");
function parseArgs(args) {
  var _a, _b;
  const { input, output, expected, context, ...clientArgs } = args;
  const chatArgs = {
    model: (_a = args.model) != null ? _a : DEFAULT_MODEL,
    temperature: (_b = args.temperature) != null ? _b : 0
  };
  if (args.maxTokens) {
    chatArgs.max_tokens = args.maxTokens;
  }
  return {
    input,
    output,
    expected,
    context: flatenContext(context),
    chatArgs,
    client: buildOpenAIClient(clientArgs)
  };
}
function flatenContext(context) {
  return context === void 0 ? context : Array.isArray(context) ? context.join("\n") : context;
}
function checkRequired(args, name) {
  for (const [key, value] of Object.entries(args)) {
    if (value === void 0) {
      throw new Error(`${name} requires ${key} value`);
    }
  }
  return args;
}
function mustParseArgs(resp) {
  var _a, _b, _c;
  const args = (_c = (_b = (_a = resp.choices[0]) == null ? void 0 : _a.message.tool_calls) == null ? void 0 : _b[0]) == null ? void 0 : _c.function.arguments;
  if (!args) {
    throw new Error("No tool call returned");
  }
  return JSON.parse(args);
}

// js/value.ts
var ExactMatch = makePartial(
  (args) => {
    var _a, _b;
    const maybeObject = needsJSON(args.output) || needsJSON(args.expected);
    const [output, expected] = [
      normalizeValue((_a = args.output) != null ? _a : null, maybeObject),
      normalizeValue((_b = args.expected) != null ? _b : null, maybeObject)
    ];
    const score = output === expected ? 1 : 0;
    return {
      name: "ExactMatch",
      score
    };
  },
  "ExactMatch"
);
function needsJSON(value) {
  return typeof value === "object" || Array.isArray(value);
}
function normalizeValue(value, maybeObject) {
  if (needsJSON(value)) {
    return JSON.stringify(value);
  }
  try {
    if (typeof value === "string" && maybeObject) {
      return JSON.stringify(JSON.parse(value));
    }
  } catch (e) {
  }
  return `${value}`;
}

// js/manifest.ts
var Evaluators = [
  {
    label: "LLM-as-a-Judge",
    methods: [
      {
        method: Battle,
        description: "Test whether an output _better_ performs the `instructions` than the original (expected) value.",
        template: templates.battle,
        requiresExtraParams: true
      },
      {
        method: ClosedQA,
        description: "Test whether an output answers the `input` using knowledge built into the model. You can specify `criteria` to further constrain the answer.",
        template: templates.closed_q_a,
        requiresExtraParams: true
      },
      {
        method: Humor,
        description: "Test whether an output is funny.",
        template: templates.humor
      },
      {
        method: Factuality,
        description: "Test whether an output is factual, compared to an original (`expected`) value.",
        template: templates.factuality
      },
      {
        method: Moderation,
        description: "A scorer that uses OpenAI's moderation API to determine if AI response contains ANY flagged content."
      },
      {
        method: Possible,
        description: "Test whether an output is a possible solution to the challenge posed in the input.",
        template: templates.possible
      },
      {
        method: Security,
        description: "Test whether an output is malicious.",
        template: templates.security
      },
      {
        method: Sql,
        description: "Test whether a SQL query is semantically the same as a reference (output) query.",
        template: templates.sql
      },
      {
        method: Summary,
        description: "Test whether an output is a better summary of the `input` than the original (`expected`) value.",
        template: templates.summary
      },
      {
        method: Translation,
        description: "Test whether an `output` is as good of a translation of the `input` in the specified `language` as an expert (`expected`) value.",
        template: templates.translation,
        requiresExtraParams: true
      }
    ]
  },
  {
    label: "RAG",
    methods: [
      {
        method: ContextEntityRecall,
        description: "Estimates context recall by estimating TP and FN using annotated answer and retrieved context.",
        requiresExtraParams: true
      },
      {
        method: ContextRelevancy,
        description: "Extracts relevant sentences from the provided context that are absolutely required to answer the given question.",
        requiresExtraParams: true
      },
      {
        method: ContextRecall,
        description: "Analyzes each sentence in the answer and classifies if the sentence can be attributed to the given context or not.",
        requiresExtraParams: true
      },
      {
        method: ContextPrecision,
        description: "Verifies if the context was useful in arriving at the given answer.",
        requiresExtraParams: true
      },
      {
        method: AnswerRelevancy,
        description: "Scores the relevancy of the generated answer to the given question.",
        requiresExtraParams: true
      },
      {
        method: AnswerSimilarity,
        description: "Scores the semantic similarity between the generated answer and ground truth.",
        requiresExtraParams: true
      },
      {
        method: AnswerCorrectness,
        description: "Measures answer correctness compared to ground truth using a weighted average of factuality and semantic similarity.",
        requiresExtraParams: true
      }
    ]
  },
  {
    label: "Composite",
    methods: [
      {
        method: ListContains,
        description: "Semantically evaluates the overlap between two lists of strings using pairwise similarity and Linear Sum Assignment."
      },
      {
        method: ValidJSON,
        description: "Evaluates the validity of JSON output, optionally validating against a JSON Schema definition."
      }
    ]
  },
  {
    label: "Embeddings",
    methods: [
      {
        method: EmbeddingSimilarity,
        description: "Evaluates the semantic similarity between two embeddings using cosine distance."
      }
    ]
  },
  {
    label: "Heuristic",
    methods: [
      {
        method: JSONDiff,
        description: "Compares JSON objects using customizable comparison methods for strings and numbers."
      },
      {
        method: Levenshtein,
        description: "Uses the Levenshtein distance to compare two strings."
      },
      {
        method: ExactMatch,
        description: "Compares two values for exact equality. If the values are objects, they are converted to JSON strings before comparison."
      },
      {
        method: NumericDiff,
        description: "Compares numbers by normalizing their difference."
      }
    ]
  }
];
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  AnswerCorrectness,
  AnswerRelevancy,
  AnswerSimilarity,
  Battle,
  ClosedQA,
  ContextEntityRecall,
  ContextPrecision,
  ContextRecall,
  ContextRelevancy,
  DEFAULT_MODEL,
  EmbeddingSimilarity,
  Evaluators,
  ExactMatch,
  Factuality,
  Faithfulness,
  Humor,
  JSONDiff,
  LLMClassifierFromSpec,
  LLMClassifierFromSpecFile,
  LLMClassifierFromTemplate,
  Levenshtein,
  LevenshteinScorer,
  ListContains,
  Moderation,
  NumericDiff,
  OpenAIClassifier,
  Possible,
  Security,
  Sql,
  Summary,
  Translation,
  ValidJSON,
  buildClassificationTools,
  init,
  makePartial,
  modelGradedSpecSchema,
  normalizeValue,
  templates
});
